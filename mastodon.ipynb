{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77192667",
   "metadata": {},
   "source": [
    "# Scrapper for Data Acquisition from Mastodon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f9feb",
   "metadata": {},
   "source": [
    "## https://mastodon.social/explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195dccf4",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db28a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "\n",
    "# Set up the Chrome webdriver\n",
    "chrome_driver_path = \"D:\\\\New\\\\Data Science\\\\Assignment_1\\\\Scrapper_3\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Navigate to the Mastodon explore page\n",
    "driver.get(\"https://mastodon.social/explore\")\n",
    "\n",
    "# Define the number of posts to scrape\n",
    "num_posts_to_scrape = 3\n",
    "posts_scraped = 0\n",
    "\n",
    "# Create a directory to store downloaded images\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "# Scroll to load more posts\n",
    "while posts_scraped < num_posts_to_scrape:\n",
    "    # Scroll down to load more posts\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "    # Find all images on the page\n",
    "    images = driver.find_elements_by_xpath(\"//img\")\n",
    "\n",
    "    # Loop through each image\n",
    "    for image in images:\n",
    "        try:\n",
    "            # Get the image source\n",
    "            image_url = image.get_attribute(\"src\")\n",
    "\n",
    "            # Download the image\n",
    "            if image_url:\n",
    "                image_filename = f\"images/image_{posts_scraped + 1}.jpg\"\n",
    "                with open(image_filename, \"wb\") as f:\n",
    "                    f.write(requests.get(image_url).content)\n",
    "                print(f\"Downloaded image: {image_filename}\")\n",
    "                posts_scraped += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image: {str(e)}\")\n",
    "\n",
    "        if posts_scraped >= num_posts_to_scrape:\n",
    "            break\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339105b4",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb5a3e9",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Set up the Chrome webdriver\n",
    "chrome_driver_path = \"D:\\\\New\\\\Data Science\\\\Assignment_1\\\\Scrapper_3\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Navigate to the Mastodon explore page\n",
    "driver.get(\"https://mastodon.social/explore\")\n",
    "\n",
    "# Define the number of posts to scrape\n",
    "num_posts_to_scrape = 3\n",
    "posts_scraped = 0\n",
    "\n",
    "# Scroll to load more posts\n",
    "while posts_scraped < num_posts_to_scrape:\n",
    "    # Scroll down to load more posts\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Find all text content within each post\n",
    "    posts = soup.find_all(\"div\", class_=\"status__content\")\n",
    "\n",
    "    for post_index, post in enumerate(posts, start=1):\n",
    "        try:\n",
    "            # Extract text content from the post\n",
    "            post_text = post.get_text(separator='\\n').strip()\n",
    "\n",
    "            # Save text content to a text file\n",
    "            text_filename = f\"text/post_{posts_scraped + post_index}.txt\"\n",
    "            with open(text_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(post_text)\n",
    "            print(f\"Saved text content to: {text_filename}\")\n",
    "\n",
    "            posts_scraped += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing post: {str(e)}\")\n",
    "\n",
    "        if posts_scraped >= num_posts_to_scrape:\n",
    "            break\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a85a43",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc98afb6",
   "metadata": {},
   "source": [
    "## Audio Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "\n",
    "# Set up the Chrome webdriver\n",
    "chrome_driver_path = \"D:\\\\New\\\\Data Science\\\\Assignment_1\\\\Scrapper_3\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "# Navigate to the audio page\n",
    "driver.get(\"https://mastodon.social/explore\")\n",
    "\n",
    "# Define the number of audio files to scrape\n",
    "num_audio_to_scrape = 1\n",
    "audio_scraped = 0\n",
    "\n",
    "# Create an audio directory if it doesn't exist\n",
    "audio_directory = \"audio\"\n",
    "os.makedirs(audio_directory, exist_ok=True)\n",
    "\n",
    "# CSV file path\n",
    "csv_filename = os.path.join(audio_directory, \"audio_links.csv\")\n",
    "csv_exists = os.path.exists(csv_filename)\n",
    "\n",
    "# Open CSV file in append mode\n",
    "with open(csv_filename, 'a', newline='') as csvfile:\n",
    "    fieldnames = ['Audio URL']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    # Write header only if the file doesn't exist\n",
    "    if not csv_exists:\n",
    "        writer.writeheader()\n",
    "\n",
    "    # Find all audio elements on the page\n",
    "    audio_elements = driver.find_elements_by_xpath(\"//audio/source\")\n",
    "\n",
    "    # Loop through each audio element\n",
    "    for audio in audio_elements:\n",
    "        try:\n",
    "            # Get the audio source\n",
    "            audio_url = audio.get_attribute(\"src\")\n",
    "\n",
    "            # Check if the URL ends with .mp3, .wav, or .ogg\n",
    "            if audio_url and audio_url.endswith(('.mp3', '.wav', '.ogg')):\n",
    "                # Write audio URL to CSV\n",
    "                writer.writerow({'Audio URL': audio_url})\n",
    "                print(f\"Audio URL added to CSV: {audio_url}\")\n",
    "                audio_scraped += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing audio: {str(e)}\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16444f03",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc8ba2b",
   "metadata": {},
   "source": [
    "## Video Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "\n",
    "def scroll_page(driver):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)  # Wait for the page to load\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Set up the Chrome webdriver\n",
    "        chrome_driver_path = \"D:\\\\New\\\\Data Science\\\\Assignment_1\\\\Scrapper_3\\\\chromedriver.exe\"\n",
    "        driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "        # Navigate to the Mastodon explore page\n",
    "        driver.get(\"https://mastodon.social/explore/\")\n",
    "\n",
    "        # Define the number of posts to scrape\n",
    "        num_posts_to_scrape = 1\n",
    "        posts_scraped = 0\n",
    "\n",
    "        # Create a directory to store downloaded video links\n",
    "        videos_folder = \"videos\"\n",
    "        os.makedirs(videos_folder, exist_ok=True)\n",
    "\n",
    "        # Text file to store video links\n",
    "        txt_filename = os.path.join(videos_folder, \"video_links.txt\")\n",
    "\n",
    "        # Scroll to load more posts\n",
    "        while posts_scraped < num_posts_to_scrape:\n",
    "            # Scroll down to load more posts\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "            # Parse the HTML content using BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # Find all video content within each post\n",
    "            videos = soup.find_all(\"video\")\n",
    "\n",
    "            with open(txt_filename, \"a\", encoding=\"utf-8\") as txt_file:\n",
    "                for video in videos:\n",
    "                    try:\n",
    "                        # Extract video links from the video tag\n",
    "                        video_url = video.get(\"src\")\n",
    "                        if video_url:\n",
    "                            # Write video links to the text file\n",
    "                            txt_file.write(video_url + \"\\n\")\n",
    "                            print(f\"Saved video link: {video_url}\")\n",
    "\n",
    "                            posts_scraped += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing video link: {str(e)}\")\n",
    "\n",
    "                    if posts_scraped >= num_posts_to_scrape:\n",
    "                        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "\n",
    "    finally:\n",
    "        # Close the browser\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38be9c6e",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd1840e",
   "metadata": {},
   "source": [
    "# Medium Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a6986d",
   "metadata": {},
   "source": [
    "## https://medium.com/@waqasdost/scraping-data-from-mastodon-social-explore-page-a8fb00eba5a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02553cf6",
   "metadata": {},
   "source": [
    "## Citation: Idea Learing from DataCamp and youtube, Code by ChatGPT3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6292a35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa66dfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
